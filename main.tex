%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[11pt, twosides]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xcolor}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
%   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS 419M Introduction to Machine Learning
                        \hfill Spring 2021-22} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
% \renewcommand{\cite}[1]{[#1]}
% \def\beginrefs{\begin{list}%
%         {[\arabic{equation}]}{\usecounter{equation}
%          \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%          \setlength{\labelwidth}{1.6truecm}}}
% \def\endrefs{\end{list}}
% \def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
% \newcommand{\fig}[3]{
%           \vspace{#2}
%           \begin{center}
%           Figure \thelecnum.#1:~#3
%           \end{center}
%   }
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
% \newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{5}{Regression}{Abir De}{Lecture 5 - Group 2}
%\lecture{x}{Title}{Abir De}{Group y}

\section{Introduction to Regression}
Let's say we are given a dataset \{(x,y)\} where $y \in \mathbb{R}$.
Till now we have considered $y \in \{-1,+1\}$ but here we are considering $y \in \mathbb{R}$.

We need to find mapping from x to y that is $x \mapsto y$

\textbf{Applications of Regression}
\begin{enumerate}
    \item House price prediction 
    \item Time series prediction (predicting stocks, loans etc.)
    \item Sentiment detection
\end{enumerate}

If we are given a set a data points $(x_1,y_1) , (x_2,y_2) , (x_3,y_3), \ldots$  we need to find y given x for an unseen sample. So here y is not known and x is not present during training.

 Our goal is to come up with some function h(x) so that $h(x) \approx y$.
 

\section{When h(x) is linear}
If the function $h(x)$ is linear, then 
    \[h(x) = w^T x\]

For each data point $(x_i, y_i)$, we get $y_i \approx w^T x$, i.e, 
\begin{eqnarray*}
    [y_1, y_2, \hdots, y_n] &=& w^T [x_1, x_2, \hdots, x_n]\\
    Y_{1\times n} &=& w^T_{1\times d} X_{d\times n}
\end{eqnarray*}

To get w, we need to solve the above equation. So, for solution to exist, 
\[Y\in \mathbb{R}(X)\]

We have 
    \[rank(X) = rank(X^T) \leq d\]

Since $d<<n$, columns of $X^T$ cannot span entire $R^n$ and as y is n-dimensional vector, we can have $y\in \mathbb{R}^n \backslash \mathbb{R}(X)$. Therefore, solution may not exist.\\
Also, we need to consider the noise.


\section{Existence of the inverse ${(\sum_{i \in D} (x_i x_i^T))^{-1}}$}
 
We derived : 
\begin{equation}
    \mathbf{ W^* = (\sum_{i \in D} (x_i x_i^T))^{-1} \sum_{i \in D} (x_i y_i)} 
\end{equation}

Let \textbf{V = $\sum_{i \in D} (x_i x_i^T)$} .\\
For existence of $V^{-1}$, we need that \textbf{$|V| \neq 0 $ }. Now, let us analyze what is the probability of having \textbf{$|V| = 0 $ }.\\
\begin{equation}
    \mathbf{|V| = f( x_1^1, x_1^2, . . . x_n^{d-1}, x_n^d) }
\end{equation}
, where {$x_i^j$ = $j^{th}$ element of \textbf{$x_i$}
\begin{enumerate}
    \item For two square matrices $A, B$of size $n\times n$ if $AB = BA$ for all B, then show that $A = cI_n$ for some $c \in \mathbb{R}$\\
    \color{blue}
    Pick $B$ to be a diagonal matrix with pair-wise distinct elements. Then it can be shown that $A$ is also a diagonal matrix. Now pick $B$ to be a matrix with all ones, i.e. $B = [1]_{ij}$. As $A$ is diagonal, $AB=BA$ implies all diagonal entries of $A$ are equal i.e., $A = cI_n$ for some $c \in \mathbb{R}$
    \color{black}
    \item If $x^\top A x = 0\:\: \forall x \in \mathbb{R}^n$ then show that $A$ is skew-symmetric.\\
    \color{blue}
    On differentiating the above equation we get
    $$(A + A^\top)x = 0 \:\: \forall x \in \mathbb{R}^n$$
    This implies $A = -A^\top$
    \color{black}
    \item Show that $\text{rank}(AB) \leq \text{rank}(A)$\\
    \color{blue} Each column of $AB$ can be viewed as a linear combination of columns of $A$. Hence, if the dimension of column space of $A$ is $r$, the dimension of column space of $AB$ cannot be more than $r$. In other words,
    $\text{rank}(AB) \leq \text{rank}(A)$
    \color{black}
    \item Suppose you have a uniform sampler which samples uniformly from $[0, 1]$. Propose an algorithm which uses this uniform sampler to generate samples from any given distribution.\\
    \color{blue}
    Suppose we have a uniform random variable $U \sim \text{Uniform}([0, 1])$. We need to find a function $g$ such that the PDF of the random variable $g(U)$ will be same as that of the given distribution, say $f$. That is $g(U) \sim f$. Which is same as
    \begin{eqnarray*}
    \mathbb{P}(g(U) \leq x) &=& \mathbb{P}(U \leq g^{-1}(x))\\
    \implies \int \limits_{-\infty}^x f(x) \, dx &=& \int \limits_{-\infty}^{g^{-1}(x)} 1 \, du\\
    \implies F(x) &=& g^{-1}(x)\\
    \implies g &=& F^{-1}
    \end{eqnarray*}
    \color{black}
\end{enumerate}

\section{Regularization}
We saw in the previous section, using the notion of probability of a multi-variable function's zeros being obtained from a random distribution to be $0$, that the probability of $det(x_ix_i^T)$ being $0$ was negligible.
\color{blue}
\begin{align*}
    |\sum_{i \in D} x_ix_i^T| &= f(x_{1i},x_{2i},x_{3i}...,x_{dd})\\
    Pr(f(x_{1i},x_{2i},x_{3i}...,x_{dd})) &\rightarrow 0\\
    \therefore Pr(|\sum_{i \in D} x_ix_i^T|) &\rightarrow 0\\
    \therefore (\sum_{i \in D}x_ix_i^T)^{-1}\ &\ exists\\
\end{align*}
\color{black}
However, we may see that the above function may have a very small value with finite, non-zero probability. In fact, if we have all $x_{ij} ~ N(0,1)$, the probability that: 
\color{blue}
\begin{equation*}
|\sum_{i \in D} x_ix_i^T| \in [-\epsilon,\epsilon]\end{equation*}
\color{black} is significant.\\

The issue with this is that according to the previously derived equation for $\omega*$, if the aforementioned determinant is very small, then the inverse of $\sum_{i \in D} x_ix_i^T$ would be very large. This causes the issue that even for a small point $(x_i, y_i)$, the value of $\omega$ would be very large, thus making the equation numerically unstable.\\

Hence, we employ the method of {\bf Regularization}; whereby we obtain the following equation for the ideal parameter $\omega$, by adding an extra hyperparameter $\lambda$ to our optimization equation, to obtain\\
\begin{equation*}
    \omega_2 = (\lambda I + \sum_{i \in D} x_ix_i^T)^{-1} . \sum_{i \in D} x_iy_i
\end{equation*}

Clearly, if the value of $\lambda$ is large enough, then the value of $\omega$ will be numerically stable.\\

However, how large or small should the value of $\lambda$ be?\\
For this, let us take an example scenario, where we have only one sample, that is, $|D| = 1$.
\begin{align*}
\color{blue}
    L(w) &= \sum_{i \in D} (y_i - \omega_2^T x_i)^2\\
    \therefore L(w) &= (y_1 - \omega_2^T x_1)^2\\
    \\ 
    \color{black}
    \lambda \rightarrow \infty\\ 
    \color{blue}
    \therefore \omega_2 \rightarrow 0\\
    \therefore L(w) \rightarrow y_1^2\\
    \\ 
    \color{black}
    \lambda \rightarrow 0\\ 
    \color{blue}
    \therefore \omega_2 \rightarrow (x_1x_1^T)^{-1}\\
    \therefore L(w) \rightarrow 0\\
\end{align*}
\color{black}
However, do note that for a dataset of just a single sample, $x_1x_1^T$ would clearly not be invertible, because the rank of $x_1x_1^T$ is 1\\
Hence, we need to take care of how we set the regularization constant, because if it is higher, then the loss function would not return a value of $0$, but if it is too small, then the previous problem of the matrix being non-invertible may creep up.\\

Now, let us try to find the optimization function for which we obtain $\omega_2$ as the solution.\\

We already know that the initial optimization problem was given by
\begin{equation*}
    \min_{\omega} \sum_{i \in D} (y_i - \omega^T x_i)^2 
\end{equation*}

The solution for the above problem was given by
\begin{equation*}
    \omega_1 = (\sum_{i \in D} x_ix_i^T)^{-1} \sum_{i \in D} x_iy_i
\end{equation*}

Now, for the equation obtained after regularization
\begin{align*}
    \omega_2 &= (\lambda I + \sum_{i \in D} x_ix_i^T)^{-1} \sum_{i \in D} x_iy_i \\
    \therefore 2\lambda I \omega_2 + 2\sum_{i \in D} x_ix_i^T \omega &= 2\sum_{i \in D} x_iy_i\\
    \therefore 2\lambda I \omega_2 + 2\sum_{i \in D} x_ix_i^T \omega - 2\sum_{i \in D} x_iy_i &= 0\\
    \therefore \frac{d}{d\omega} (\omega^T (\lambda I) \omega + \sum_{i \in D} (y_i^2 + \omega^T x_ix_i^T \omega - 2\omega^Tx_iy_i)) &= 0\\
    \therefore \frac{d}{d\omega} (\sum_{i \in D} (y_i^2 + \omega^T x_ix_i^T \omega - 2\omega^Tx_iy_i) + \lambda ||\omega||^2) &= 0\\
    \therefore \frac{d}{d\omega} (\sum_{i \in D} (y_i - \omega^T x_i)^2 + \lambda ||\omega||^2) &= 0
\end{align*}
Hence, we obtain that the optimization problem for the given $\omega_2$ obtained after regularization is given by:\\
\color{blue}
\begin{equation*}
    \min_{\omega} \sum_{i \in D} (y_i - \omega^Tx_i)^2 + \lambda ||\omega||^2
\end{equation*}
\color{black}
\section{Group Details and Individual Contribution}
% Fill this part
\begin{enumerate}
    \item {\bf Mayank Gupta}:\\
    \\
    \item {\bf Malhar Kulkarni}: {\bf Regularization} to prevent determinant of $x_ix_i^T$ to be too small\\
    \\
    \item {\bf N Vishal}: When $h(x)$ is linear (section 5.2)\\
    \\
    \item {\bf Pradyumna atreya}:\\
    \\
    \item {\bf Tanisha Khandelwal}: Introduction to Regression (section 5.1)\\
    \\
\end{enumerate}
\end{document}





